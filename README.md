# Distributed, Causally Consistent, Sharded Key-Value Store

##### Written in Golang using HTTP requests

## Mechanism Description
* Causal dependency tracking
     * To track causal metadata, I used a system of vector clocks along with the causal broadcast protocol that was discussed in class. To implement a vector clock, I used a Go map structure from a string to an integer, with the string keys being the addresses seen in the view and the integers being the corresponding vector clock value. Then, I implemented the causal broadcast protocol, and used HTTP requests to a separately made endpoint, /replica/, to update other replicas in the system. The key to handling vector clocks was making separate auxiliary functions to help compare their values and convert between different data types.
* Downed replica detection
     * I initially considered using a separate routine to continuously check on the health of the replicas (in the same vein as Heartbeat https://martinfowler.com/articles/patterns-of-distributed-systems/heartbeat.html). However, I settled on simply checking to see whether replicas were reachable on /kvs requests. Anytime a /kvs request is made, a GET request is broadcast to each replica currently in the View. The request is made to an "internal" endpoint /store, which simply returns the KV pairs in the replica's store. If the GET request succeeds, then the replica is reachable and no action is needed. If the request times out or is otherwise unsuccessful, it is assumed that the replica has gone down. At this point a DELETE /view is broadcast to all replicas except the downed one, which removes the unreachable replica from the other replica's views. False positives might occur if a view is simply slow to reach rather than completely down, while false negatives may might occur if a view is brought down and then online in a sufficiently fast manner.
* Key-to-shard mapping
     * To map keys to shards within the store, I partitioned by the hash of the key due to simplicity. I then had to modify our causal broadcast dependency function to ignore causal metadata from other shards.
* Resharding Mechanism
     * To reshard the nodes, I created a method so that when a node receives a resharding request, it first performs a for-loop over the VIEWS array to assign views to a shard by placing it in a map representing all shards (shards are integers keys, while their values are string arrays consisting of the socket addresses of each node within the shard). Because all nodes should have the same view and shard count, each node will arrive at the same mapping for shards to nodes. Once resharding is completed, each node must update its store as its key-value pairs may now belong to a different shard. Each node checks its key-value pairs and identifies whether it belongs in the current shard or not. If it does, the node broadcasts the pair to every node in the same shard so that they are all guaranteed to have the pair in their store. If it belongs to a different shard, the node sends the pair to every node in that shard and deletes it from its own store. Using this strategy, we have a distinct separation between resharding nodes and rebalancing key-value pairs which allows for ease of development and troubleshooting. The same logic as resharding is used for initial sharding, except there is no need to rebalance keys.
